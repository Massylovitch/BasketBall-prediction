{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e580794d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "print(os.path.basename(os.getcwd()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22180ec7-64a1-4341-bb0f-bf03a71f6044",
   "metadata": {},
   "outputs": [],
   "source": [
    "OPTUNA = True  # set to True to run Optuna first, false to use saved hyperparameters\n",
    "# MODEL_NAME = \"xgboost\"\n",
    "MODEL_NAME = \"lightgbm\"\n",
    "\n",
    "# check if being run direct (from notebooks folder) or run indirect (called from notebook 10, which already changed working directory to root)\n",
    "import os\n",
    "\n",
    "if os.path.basename(os.getcwd()) == \"notebooks\":\n",
    "\n",
    "    INDIRECT = False\n",
    "\n",
    "    os.chdir(\n",
    "        \"..\"\n",
    "    )  ## change working directory to project root when running from notebooks folder to make it easier to import modules and to access sibling folders\n",
    "\n",
    "    from dotenv import (\n",
    "        load_dotenv,\n",
    "    )  # load environment variables from .env file. If being run from notebook 10, this will already have been done\n",
    "\n",
    "    load_dotenv()\n",
    "\n",
    "else:\n",
    "    INDIRECT = True\n",
    "\n",
    "INDIRECT"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6ffc0baa-cdf2-4062-91a2-c654f0974cb2",
   "metadata": {},
   "source": [
    "## Model Testing\n",
    "\n",
    "Runs Gradient-Boosted Tree (gbt) training models (option to select Xgboost or Lightgbm) and logs the key information to Neptune.ai. \n",
    "\n",
    "(Originally 2 notebooks were utilized, one for XGB and one for LGB, but these were converted to this single notebook for convenience. Simple \"if\" statements are used to select the small differences in code when choosing one or the other.)\n",
    "\n",
    "If Optuna is set to True, then hyperparameters are tuned first and used for the test run. Otherwise, the current best hyperparameters are kept in a JSON file and are utilized instead. If Optuna is utilized, then a separate Neptune logging run is initialized to record the key tuning data.\n",
    "\n",
    "Various metrics are calculated and recorded. AUC will likely be of primary interest since the eventual goal is to compare win-lose probabilities against betting odds, not necessarily to accurately predict the winner for each game. Accuracy is an interesting metric though, and is recorded as well.\n",
    "\n",
    "Along with standard built-in measures for feature importance using weight and gain, Shapley value feature importances are also generated to give a different perspective on feature importances. These Shapley feature importances are local to the specific data run through the model and therefore can be used in some form of adversarial evaluation, such as train data vs test data and/or test split 1 vs test split 2 (see below.)\n",
    "\n",
    "This model evaluation does include splitting the test set into \"early\" season data (Test1) and \"later\" season data (Test2) and comparing the performance on each.\n",
    "\n",
    "Process flow:\n",
    "\n",
    "- load data that has gone through feature engineering and selection\n",
    "- initialize Neptune.ai logging run\n",
    "- set key options\n",
    "- fix datatypes for correct date format and decrease memory footprint\n",
    "- perform any categorical encoding required for XGB or LGB\n",
    "- drop any features that are not useful and set the target\n",
    "- load hyperparameters from JSON file or re-tune them with Optuna\n",
    "- train the model with stratified K-fold cross validation\n",
    "- output key metrics and feature importances for Out-of-Fold validation set\n",
    "- run the model on the test/validation set\n",
    "- output key metrics and feature importances for test/validation data\n",
    "- split the test/validation data into Test1 (early season) and Test2 (later season)\n",
    "- run model on each\n",
    "- output key metrics and feature importances for each\n",
    "- perform model evaluation, comparing train vs test and test1 vs test2\n",
    "- if re-tuned hyperparameters are better (per human inspection) then manually run the function to save these as new defaults. (These re-tuned hyperparameters are always logged at Neptune.ai and can be retrieved whenever necessary, but human interaction is required to establish new default parameters because various experiments might not be optimal in the long run.)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f71787a4-84ae-44fc-bcaf-ea41b56807c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "pd.set_option('display.max_columns', 500)\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    roc_auc_score,\n",
    "    roc_curve,\n",
    "    confusion_matrix,\n",
    "    classification_report,\n",
    "    ConfusionMatrixDisplay\n",
    ")\n",
    "\n",
    "\n",
    "from sklearn.model_selection import (\n",
    "    StratifiedKFold, \n",
    "    TimeSeriesSplit,\n",
    ")\n",
    "\n",
    "from sklearn.calibration import (   \n",
    "    calibration_curve,\n",
    "    CalibratedClassifierCV,\n",
    ")\n",
    "\n",
    "\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBClassifier\n",
    "print('XGB version:', xgb.__version__)\n",
    "\n",
    "import lightgbm as lgb\n",
    "from lightgbm import LGBMClassifier\n",
    "from lightgbm import (\n",
    "    early_stopping,\n",
    "    log_evaluation,\n",
    ")\n",
    "print('LGB version:', lgb.__version__)\n",
    "\n",
    "import optuna\n",
    "from optuna.visualization import (\n",
    "    plot_optimization_history, \n",
    "    plot_param_importances,\n",
    ")\n",
    "\n",
    "from src.models.model1.hyperparameters_tuning import (\n",
    "    XGB_objective,\n",
    "    LGB_objective,\n",
    ")\n",
    "\n",
    "import neptune.new as neptune\n",
    "from neptune.integrations.xgboost import NeptuneCallback\n",
    "from neptune.integrations.lightgbm import (\n",
    "    NeptuneCallback as LGB_NeptuneCallback, \n",
    ")\n",
    "\n",
    "import neptune.integrations.optuna as optuna_utils\n",
    "from neptune.types import File\n",
    "\n",
    "import joblib\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import shap\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "\n",
    "from pathlib import Path  #for Windows/Linux compatibility\n",
    "\n",
    "import json\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "from src.data.cleaning import (\n",
    "    process_games,\n",
    "    add_TARGET,\n",
    "    split_train_test,\n",
    ")\n",
    "\n",
    "from src.data.build_features import (\n",
    "    fix_datatypes,\n",
    "    process_features,\n",
    "    remove_non_rolling,\n",
    ")\n",
    "\n",
    "from src.models.model1.train import (\n",
    "    encode_categoricals,\n",
    "    plot_calibration_curve,\n",
    "    calculate_classification_metrics\n",
    ")\n",
    "\n",
    "from src.utils.constants import (\n",
    "    LONG_INTEGER_FIELDS,\n",
    "    SHORT_INTEGER_FIELDS,\n",
    "    DATE_FIELDS,\n",
    "    DROP_COLUMNS,\n",
    "    CATEGORY_COLUMNS,\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')\n",
    "\n",
    "#%config InlineBackend.figure_format = 'svg'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1c1639a",
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIGS_PATH = Path.cwd() / \"configs\"\n",
    "DATA_PATH = Path.cwd() / \"data\"\n",
    "NOTEBOOKS_PATH = Path.cwd() / \"notebooks\"\n",
    "MODELS_PATH = Path.cwd() / \"models\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "306bff41",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_training_data():\n",
    "\n",
    "    # when this notebook is run from notebook 10_model_training.pipeline, it handles retrieving the most current data from Hopsworks.AI\n",
    "    # this function here is used to update the training data when one wishes to use local data stored in games.csv instead of connecting to Hopsworks.AI\n",
    "    # games.csv will need to be first updated itself if the most current data is desired\n",
    "    # all the pre-processing steps are run again, and the train and test sets are saved to the data folder\n",
    "\n",
    "    games = pd.read_csv(DATA_PATH / \"interim\" / \"games.csv\")\n",
    "    games = process_games(games)\n",
    "    games = add_TARGET(games)\n",
    "    games = process_features(games)\n",
    "    train, test = split_train_test(games)\n",
    "    train.to_csv(DATA_PATH / \"processed\" / \"train_features.csv\", index=False)\n",
    "    test.to_csv(DATA_PATH / \"processed\" / \"test_features.csv\", index=False)\n",
    "    train.to_csv(DATA_PATH / \"processed\" / \"train_selected.csv\", index=False)\n",
    "    test.to_csv(DATA_PATH / \"processed\" / \"test_selected.csv\", index=False)\n",
    "\n",
    "\n",
    "if INDIRECT == False:  # if being run direct, update training data first\n",
    "    update_training_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2f46e0f-af90-49eb-a7b6-16ba9020744b",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_NAME = \"train_selected.csv\"\n",
    "TEST_NAME = \"test_selected.csv\"\n",
    "\n",
    "train = (\n",
    "    pd.read_csv(DATA_PATH / \"processed\" / TRAIN_NAME)\n",
    "    .sample(frac=0.3)\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "test = pd.read_csv(DATA_PATH / \"processed\" / TEST_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64b6ce3c-95d9-4545-8ab8-49fae4bcb29b",
   "metadata": {},
   "source": [
    "**Setup Neptuna.ai experiment tracking**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39b5bf64-27eb-4727-926c-03148032b4da",
   "metadata": {},
   "outputs": [],
   "source": [
    "LOGGING_NOTE = \"pipeline test\"\n",
    "\n",
    "try:\n",
    "\n",
    "    NEPTUNE_API_TOKEN = os.getenv(\"NEPTUNE_API_TOKEN\")\n",
    "except:\n",
    "    raise Exception(\"Set environment variable NEPTUNE_API_TOKEN\")\n",
    "\n",
    "PROJECT = \"massyl/nba-match-prediction\"\n",
    "PROJECT_OPTUNA = (\n",
    "    \"massyl/nba-match-prediction\"  # for 2nd run if hyperparameters are tuned\n",
    ")\n",
    "SOURCE = \"07_model_testing.ipynb\"\n",
    "\n",
    "\n",
    "run = neptune.init_run(\n",
    "    project=PROJECT,\n",
    "    source_files=[SOURCE],\n",
    "    api_token=NEPTUNE_API_TOKEN,\n",
    ")\n",
    "if MODEL_NAME == \"xgboost\":\n",
    "    neptune_callback = NeptuneCallback(run=run)\n",
    "if MODEL_NAME == \"lightgbm\":\n",
    "    neptune_callback = LGB_NeptuneCallback(run=run)\n",
    "\n",
    "run[\"note\"] = LOGGING_NOTE\n",
    "run[\"sys/tags\"].add(\n",
    "    [\n",
    "        MODEL_NAME,\n",
    "    ]\n",
    ")\n",
    "run[\"dataset/train\"] = TRAIN_NAME\n",
    "run[\"dataset/test\"] = TEST_NAME"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d304655-f609-4512-b107-049cbeeb62a5",
   "metadata": {},
   "source": [
    "**Options**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e508983c-17ad-4912-bf11-0916359d6a33",
   "metadata": {},
   "outputs": [],
   "source": [
    "run[\"model/parameters/OPTUNA\"] = OPTUNA\n",
    "\n",
    "OPTUNA_CV = \"TimeSeriesSplit\"\n",
    "# OPTUNA_CV = \"StratifiedKFold\"\n",
    "\n",
    "if OPTUNA:\n",
    "    run[\"model/optuna/optuna_cv\"] = OPTUNA_CV\n",
    "    run[\"model/optuna/optuna_folds\"] = OPTUNA_FOLDS = 5\n",
    "    run[\"model/optuna/optuna_trials\"] = OPTUNA_TRIALS = 5  # 150\n",
    "\n",
    "run[\"model/parameters/k_folds\"] = K_FOLDS = 5\n",
    "run[\"model/parameters/seed\"] = SEED = 13\n",
    "run[\"model/parameters/num_boost_round\"] = NUM_BOOST_ROUND = 2000  # xgb param\n",
    "run[\"model/parameters/enable_categorical\"] = ENABLE_CATEGORICAL = False\n",
    "run[\"model/parameters/early_stopping\"] = EARLY_STOPPING = 200\n",
    "\n",
    "VERBOSITY = 0  # xgb param\n",
    "LGB_VERBOSITY = -1  # lgb param\n",
    "VERBOSE_EVAL = False  # lgb param\n",
    "LOG_EVALUATION = 10000  # lgb display parameter\n",
    "\n",
    "# lgb train params\n",
    "if MODEL_NAME == \"lightgbm\":\n",
    "    CALLBACKS = [\n",
    "        log_evaluation(LOG_EVALUATION),\n",
    "    ]  # early_stopping(EARLY_STOPPING,verbose=False),]\n",
    "\n",
    "if MODEL_NAME == \"xgboost\":\n",
    "\n",
    "    BASE_MODEL = xgb\n",
    "\n",
    "    STATIC_PARAMS = {\n",
    "        \"seed\": SEED,\n",
    "        \"eval_metric\": \"auc\",\n",
    "        \"objective\": \"binary:logistic\",\n",
    "        \"verbosity\": VERBOSITY,\n",
    "    }\n",
    "\n",
    "if MODEL_NAME == \"lightgbm\":\n",
    "\n",
    "    BASE_MODEL = lgb\n",
    "\n",
    "    STATIC_PARAMS = {\n",
    "        \"seed\": SEED,\n",
    "        \"verbosity\": LGB_VERBOSITY,\n",
    "        \"boosting_type\": \"gbdt\",\n",
    "        \"objective\": \"binary\",\n",
    "        \"metric\": \"auc\",\n",
    "    }"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7554abb2-d72b-45e9-b51d-f31f5aa8f3b6",
   "metadata": {},
   "source": [
    "**Fix Datatypes for smaller memory footprint and other random issues**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7de2db65-2803-45c0-831f-23423c610078",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = fix_datatypes(train, DATE_FIELDS, SHORT_INTEGER_FIELDS, LONG_INTEGER_FIELDS)\n",
    "test = fix_datatypes(test, DATE_FIELDS, SHORT_INTEGER_FIELDS, LONG_INTEGER_FIELDS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c9b8472-7cb4-4c27-911d-9b444dffeea8",
   "metadata": {},
   "source": [
    "**Encode categoricals**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbe70d03-a892-4f14-a789-11102ddec460",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = encode_categoricals(train, CATEGORY_COLUMNS, MODEL_NAME, ENABLE_CATEGORICAL)\n",
    "test = encode_categoricals(test, CATEGORY_COLUMNS, MODEL_NAME, ENABLE_CATEGORICAL)\n",
    "\n",
    "CATEGORY_COLUMNS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9b91832-9691-4cf3-a203-c6804c32c3da",
   "metadata": {},
   "source": [
    "**Drop Features**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bd9743c-82cf-44bc-a297-58d3ae6d10df",
   "metadata": {},
   "outputs": [],
   "source": [
    "target = train[\"TARGET\"]\n",
    "test_target = test[\"TARGET\"]\n",
    "test_target_original = test[\"TARGET\"]  # save for later probability calibration\n",
    "\n",
    "all_columns = remove_non_rolling(train)\n",
    "\n",
    "use_columns = [item for item in all_columns if item not in DROP_COLUMNS]\n",
    "\n",
    "\n",
    "train = train[use_columns]\n",
    "test = test[use_columns]\n",
    "test_original = test.copy()  # save for later probability calibration\n",
    "\n",
    "run[\"model/features\"].log(use_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2af54d6-05fe-4304-9897-56907ad7859c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_optuna():\n",
    "\n",
    "    # log separate Neptune run for optuna hyperameter tuning\n",
    "    run2 = neptune.init_run(\n",
    "        project=PROJECT_OPTUNA,\n",
    "        source_files=[\n",
    "            SOURCE,\n",
    "        ],\n",
    "        api_token=NEPTUNE_API_TOKEN,\n",
    "    )\n",
    "    run2[\"options/optuna_cv\"] = OPTUNA_CV\n",
    "    run2[\"options/optuna_folds\"] = OPTUNA_FOLDS\n",
    "    run2[\"options/optuna_trials\"] = OPTUNA_TRIALS\n",
    "    run2[\"options/enable_categorical\"] = ENABLE_CATEGORICAL\n",
    "    run2[\"features\"].log(use_columns)\n",
    "    run2[\"sys/tags\"].add(\n",
    "        [\n",
    "            MODEL_NAME,\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    if MODEL_NAME == \"xgboost\":\n",
    "        func = lambda trial: XGB_objective(\n",
    "            trial,\n",
    "            train,\n",
    "            target,\n",
    "            STATIC_PARAMS,\n",
    "            ENABLE_CATEGORICAL,\n",
    "            NUM_BOOST_ROUND,\n",
    "            OPTUNA_CV,\n",
    "            OPTUNA_FOLDS,\n",
    "            SEED,\n",
    "        )\n",
    "    if MODEL_NAME == \"lightgbm\":\n",
    "        func = lambda trial: LGB_objective(\n",
    "            trial,\n",
    "            train,\n",
    "            target,\n",
    "            CATEGORY_COLUMNS,\n",
    "            STATIC_PARAMS,\n",
    "            ENABLE_CATEGORICAL,\n",
    "            NUM_BOOST_ROUND,\n",
    "            OPTUNA_CV,\n",
    "            OPTUNA_FOLDS,\n",
    "            SEED,\n",
    "            EARLY_STOPPING,\n",
    "        )\n",
    "\n",
    "    study = optuna.create_study(direction=\"maximize\")\n",
    "    study.optimize(\n",
    "        func,\n",
    "        n_trials=OPTUNA_TRIALS,\n",
    "    )\n",
    "\n",
    "    # optuna_utils.log_study_metadata(study, run2)\n",
    "\n",
    "    print(\"Study Best Value:\", study.best_value)\n",
    "    print(\"Study Best Params:\", study.best_params)\n",
    "\n",
    "    plot_optimization_history(study)\n",
    "\n",
    "    # plot_param_importances(study)\n",
    "\n",
    "    run2[\"best_value\"] = study.best_value\n",
    "    run2[\"best_params\"] = study.best_params\n",
    "    run2[\"static_params\"] = STATIC_PARAMS\n",
    "\n",
    "    run2.stop()\n",
    "\n",
    "    return study.best_params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c773dcf-9d6e-485c-935d-658a9e4bf489",
   "metadata": {},
   "source": [
    "**Set Hyperparameters**\n",
    "\n",
    "Run OPTUNA or load best parameters saved in JSON file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb3eb917-63e5-4ff2-8b4a-d2fad52f199f",
   "metadata": {},
   "outputs": [],
   "source": [
    "if OPTUNA:\n",
    "    tuned_params = run_optuna()\n",
    "else:\n",
    "    with open(CONFIGS_PATH / (MODEL_NAME + \".json\")) as f:\n",
    "        tuned_params = json.loads(f.read())\n",
    "\n",
    "model_params = STATIC_PARAMS\n",
    "model_params.update(tuned_params)\n",
    "\n",
    "run[\"model/params\"] = model_params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22b2247e-ebb4-4951-aa33-30e8b881b574",
   "metadata": {},
   "source": [
    "**Setup Results table**\n",
    "\n",
    "Store key metrics for easy review at the bottom of the notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3fba08f-96b9-44ee-9227-e2d28b917d72",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame(\n",
    "    columns=[\"Label\", \"Accuracy\", \"AUC\", \"Threshold\"]\n",
    ")  # record metrics for easy comparison at the end\n",
    "\n",
    "\n",
    "# Load Simple Model results for later comparison\n",
    "# Simple Model predicts home team always wins\n",
    "def SimpleModel(test, true):\n",
    "\n",
    "    predict = np.ones((test.shape[0],))  # set all predictions to 1 (home team wins)\n",
    "    acc_score = accuracy_score(true, predict)\n",
    "    auc_score = roc_auc_score(true, predict)\n",
    "\n",
    "    return acc_score, auc_score\n",
    "\n",
    "\n",
    "acc_score, auc_score = SimpleModel(test, test_target)\n",
    "results.loc[len(results)] = [\"Simple Model\", acc_score, auc_score, \"N/A\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34042a79-5f67-4424-a2f3-4188d61c847b",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12c3b503-d1c4-4ff5-b58c-7fb6e677f563",
   "metadata": {},
   "source": [
    "**Support functions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a540cd3d-82ea-402a-9f9e-02b218e495bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_scores(target, preds):\n",
    "    # for accuracy score, prediction probabilities must be convert to binary scores (Win or Lose)\n",
    "    # determine optimum threshold for converting probabilities using ROC curve\n",
    "    # generally 0.5 works for balanced data\n",
    "    # fpr = false positive rate, tpr = true positive rate\n",
    "\n",
    "    fpr, tpr, thresholds = roc_curve(target, preds)\n",
    "    optimal_idx = np.argmax(tpr - fpr)\n",
    "    optimal_threshold = thresholds[optimal_idx]\n",
    "    preds_binary = (preds > optimal_threshold).astype(int)\n",
    "\n",
    "    acc_score = accuracy_score(target, preds_binary)\n",
    "    auc_score = roc_auc_score(target, preds)\n",
    "\n",
    "    print()\n",
    "    print(\"Scores:\")\n",
    "    print()\n",
    "    print(\"Accuracy Score:\", acc_score)\n",
    "    print(\"AUC Score:\", auc_score)\n",
    "    print(\"Optimal Threshold:\", optimal_threshold)\n",
    "\n",
    "    return preds_binary, acc_score, auc_score, optimal_threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d05938d6-96fa-48cc-ae4f-bb09dc895d3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_shapley(MODEL_NAME, model, data):\n",
    "    if MODEL_NAME == \"xgboost\":\n",
    "        shap = model.predict(data, pred_contribs=True)\n",
    "    if MODEL_NAME == \"lightgbm\":\n",
    "        shap = model.predict(data, pred_contrib=True)\n",
    "\n",
    "    return shap\n",
    "\n",
    "\n",
    "def get_shapley_interactions(MODEL_NAME, model, data):\n",
    "    if MODEL_NAME == \"xgboost\":\n",
    "        shap_interactions = model.predict(data, pred_interactions=True)\n",
    "    if MODEL_NAME == \"lightgbm\":  # not currently supported\n",
    "        shap_interactions = np.zeros(\n",
    "            (data.shape[0], data.shape[1] + 1, data.shape[1] + 1)\n",
    "        )\n",
    "\n",
    "    return shap_interactions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c66d26e-ab33-464f-82c3-aea83a741027",
   "metadata": {},
   "source": [
    "**Training with K-Fold Cross Validation**\n",
    "\n",
    "Shapley values are also generated using built-in functionality of XGB and LGB. This enables a different approach to determining feature importances, and because this is a local determination to the given dataset, it can be used for advesarial evaluation of train data vs test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2243dddf-9f3d-4104-94c3-c7255d296dcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "#initialize oof arrays including Shapley values and Shapley interaction values\n",
    "train_oof = np.zeros((train.shape[0],))\n",
    "train_oof_shap = np.zeros((train.shape[0],train.shape[1]+1))\n",
    "train_oof_shap_interact = np.zeros((train.shape[0],train.shape[1]+1,train.shape[1]+1))\n",
    "\n",
    "   \n",
    "# K-fold cross validation\n",
    "if OPTUNA_CV == \"StratifiedKFold\": \n",
    "    kf = StratifiedKFold(n_splits=K_FOLDS, shuffle=True, random_state=SEED)\n",
    "elif OPTUNA_CV == \"TimeSeriesSplit\":\n",
    "    kf = TimeSeriesSplit(n_splits=K_FOLDS)\n",
    "\n",
    "\n",
    "for f, (train_ind, val_ind) in tqdm(enumerate(kf.split(train, target))):\n",
    "    \n",
    "    train_df, val_df = train.iloc[train_ind], train.iloc[val_ind]\n",
    "    train_target, val_target = target[train_ind], target[val_ind]\n",
    "\n",
    "    if MODEL_NAME == \"xgboost\":\n",
    "        train_dmatrix = xgb.DMatrix(train_df, label=train_target,enable_categorical=ENABLE_CATEGORICAL)\n",
    "        val_dmatrix = xgb.DMatrix(val_df, label=val_target,enable_categorical=ENABLE_CATEGORICAL)\n",
    "        val_data = val_dmatrix\n",
    "       \n",
    "        model =  xgb.train(model_params, \n",
    "                           train_dmatrix, \n",
    "                           num_boost_round = NUM_BOOST_ROUND,\n",
    "                          callbacks=[neptune_callback],\n",
    "                          )\n",
    "    \n",
    "    if MODEL_NAME == \"lightgbm\":\n",
    "        train_lgbdataset = lgb.Dataset(train_df, label=train_target, categorical_feature=CATEGORY_COLUMNS)\n",
    "        val_lgbdataset = lgb.Dataset(val_df, label=val_target, reference = train_lgbdataset, categorical_feature=CATEGORY_COLUMNS)\n",
    "        val_data = val_df\n",
    "        \n",
    "        model =  lgb.train(model_params, \n",
    "                       train_lgbdataset,\n",
    "                       valid_sets=val_lgbdataset,\n",
    "                       num_boost_round = 10,#NUM_BOOST_ROUND,\n",
    "                       callbacks=CALLBACKS + [neptune_callback],\n",
    "                      )\n",
    "    \n",
    "    temp_oof = model.predict(val_data)\n",
    "    temp_oof_shap = get_shapley(MODEL_NAME, model, val_data)\n",
    "    temp_oof_shap_interact = get_shapley_interactions(MODEL_NAME, model, val_data)\n",
    "\n",
    "    train_oof[val_ind] = temp_oof\n",
    "\n",
    "    train_oof_shap[val_ind, :] = temp_oof_shap\n",
    "    train_oof_shap_interact[val_ind, :,:] = temp_oof_shap_interact\n",
    "    \n",
    "    temp_oof_binary, acc_score, auc_score, optimal_threshold = get_scores(val_target, temp_oof)\n",
    "\n",
    "# Out-of-Fold composite for train data\n",
    "\n",
    "train_oof_binary, acc_score, auc_score, optimal_threshold = get_scores(target,train_oof)\n",
    "\n",
    "run[\"train/accuracy\"] = acc_score \n",
    "run[\"train/AUC\"] = auc_score \n",
    "run[\"train/optimal_threshold\"] = optimal_threshold\n",
    "\n",
    "results.loc[len(results)] = ['Train', acc_score, auc_score, optimal_threshold]            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e22c10d-a994-4baa-a138-1459471567dc",
   "metadata": {},
   "source": [
    "**OOF Confusion Matrix**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1838f15d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_target.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dbea0bc-5e88-44c2-833d-1138f38ef6ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(target, train_oof_binary)\n",
    "print(cm)\n",
    "fig = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[[\"win\", \"lose\"]])\n",
    "run[\"train/confusion_matrix\"].append(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d910991-3dd2-4927-81a9-107a2ac04f34",
   "metadata": {},
   "source": [
    "**OOF Classification report**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5856a9c-b650-42e3-a771-138e1c78baa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "run[\"train/classification_report\"] = classification_report(target, train_oof_binary)\n",
    "print(classification_report(target, train_oof_binary))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54938474-8284-45f7-bf60-51e2f65f92c5",
   "metadata": {},
   "source": [
    "**Train Feature Importance via Weight/Splits - the number of times a feature appears in a tree**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f01ff400-2b6d-440a-868c-7d9d5e5eeec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "if MODEL_NAME == \"xgboost\":\n",
    "    IMPORTANCE_TYPE = \"weight\"\n",
    "if MODEL_NAME == \"lightgbm\":\n",
    "    IMPORTANCE_TYPE = \"split\"\n",
    "\n",
    "max_features = 25\n",
    "max_title = \"Top \" + str(max_features) + \" Feature importance - \" + IMPORTANCE_TYPE\n",
    "fig, ax = plt.subplots(1, 1, figsize=(10, 10))\n",
    "BASE_MODEL.plot_importance(\n",
    "    model,\n",
    "    importance_type=IMPORTANCE_TYPE,\n",
    "    max_num_features=max_features,\n",
    "    title=max_title,\n",
    "    ax=ax,\n",
    ")\n",
    "# run[\"train/feature_importance_\" + IMPORTANCE_TYPE].upload(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de7ce051-7ccb-49f7-b666-564199579e3f",
   "metadata": {},
   "source": [
    "**Train Feature Importance via Gain - the average gain of splits which use the feature**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70612253-a661-40ca-8584-4de93c41e1bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_features = 25\n",
    "max_title = \"Top \" + str(max_features) + \" Feature importance - Gain\"\n",
    "fig, ax = plt.subplots(1, 1, figsize=(10, 10))\n",
    "BASE_MODEL.plot_importance(\n",
    "    model, importance_type=\"gain\", max_num_features=max_features, title=max_title, ax=ax\n",
    ")\n",
    "run[\"train/feature_importance_gain\"].upload(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41fac593-df38-459f-b7d1-48a06a30258f",
   "metadata": {},
   "source": [
    "**OOF Feature Importance via Shapley values**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aded9c7-d089-4daf-9966-4c6eba368cb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# summarize the effects of all the features\n",
    "fig, ax = plt.subplots(1, 1, figsize=(10, 10))\n",
    "shap.summary_plot(train_oof_shap[:, :-1], train)\n",
    "run[\"train/shapley_summary\"].upload(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d737a84-793e-4a19-b36b-88462f4209ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(10, 10))\n",
    "shap.summary_plot(train_oof_shap[:, :-1], train[use_columns], plot_type=\"bar\")\n",
    "run[\"train/shapley_summary_bar\"].upload(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71399d78-29ad-41cf-a6df-f8e82db5e49b",
   "metadata": {},
   "source": [
    "**Save train data with predictions**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "003f2a2b-82ae-4bfa-ac23-67141ab1e538",
   "metadata": {},
   "outputs": [],
   "source": [
    "train[\"TARGET\"] = target\n",
    "train[\"PredictionPct\"] = train_oof\n",
    "train[\"Prediction\"] = train_oof_binary\n",
    "train.to_csv(DATA_PATH / \"processed\" / \"train_predictions.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab20da39-e086-4b2a-bc29-383a6bc7ff76",
   "metadata": {},
   "source": [
    "### Test Data Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "578ea911-966d-49a2-acec-a4763bd06d90",
   "metadata": {},
   "outputs": [],
   "source": [
    "if MODEL_NAME == \"xgboost\":\n",
    "    test_data = xgb.DMatrix(test, enable_categorical=ENABLE_CATEGORICAL)\n",
    "if MODEL_NAME == \"lightgbm\":\n",
    "    test_data = test\n",
    "\n",
    "test_preds = model.predict(test_data)\n",
    "test_preds_shap = get_shapley(MODEL_NAME, model, test_data)\n",
    "\n",
    "test_preds_binary, acc_score, auc_score, optimal_threshold = get_scores(\n",
    "    test_target, test_preds\n",
    ")\n",
    "\n",
    "run[\"test/accuracy\"] = acc_score\n",
    "run[\"test/AUC\"] = auc_score\n",
    "run[\"test/optimal_threshold\"] = optimal_threshold\n",
    "\n",
    "results.loc[len(results)] = [\"Test\", acc_score, auc_score, optimal_threshold]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bebc7035-973a-4008-bb60-0d34afcb25e9",
   "metadata": {},
   "source": [
    "**Test Confusion Matrix**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8c69ef4-60c5-4f42-a38f-03cfae9ba941",
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(test_target, test_preds_binary)\n",
    "print(cm)\n",
    "fig = ConfusionMatrixDisplay(cm, display_labels=[\"win\", \"lose\"])\n",
    "run[\"test/confusion_matrix\"].append(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d73e53c0-a375-4e37-a81e-c501eeac83ca",
   "metadata": {},
   "source": [
    "**Test Classification report**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a9f58b4-12ee-4997-b158-bf8d5b8d22ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "run[\"test/classification_report\"] = classification_report(\n",
    "    test_target, test_preds_binary\n",
    ")\n",
    "print(classification_report(test_target, test_preds_binary))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a97fb0e-2af4-4d77-a921-83bcdfa5205a",
   "metadata": {},
   "source": [
    "**Test Feature Importance via Shapley values**\n",
    "\n",
    "For comparison to cross-validation OOF Shapley values to ensure that the model is working in similar manner on the test data as train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6cf8e5b-6b9a-4b06-a325-d0d40e06e297",
   "metadata": {},
   "outputs": [],
   "source": [
    "# summarize the effects of all the features\n",
    "fig, ax = plt.subplots(1, 1, figsize=(10, 10))\n",
    "shap.summary_plot(test_preds_shap[:, :-1], test)\n",
    "run[\"test/shapley_summary\"].upload(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db7d4fcf-d2a9-4a8e-bd47-f59894744503",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(10, 10))\n",
    "shap.summary_plot(test_preds_shap[:, :-1], test[use_columns], plot_type=\"bar\")\n",
    "run[\"test/shapley_summary_bar\"].upload(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25b3346d-1ff1-4a46-8580-147febe591e5",
   "metadata": {},
   "source": [
    "**Save test data with predictions**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2454de3-a95b-4c6a-9e91-56c426eb4142",
   "metadata": {},
   "outputs": [],
   "source": [
    "test[\"TARGET\"] = test_target\n",
    "test[\"PredictionPct\"] = test_preds\n",
    "test[\"Prediction\"] = test_preds_binary\n",
    "test.to_csv(DATA_PATH / \"processed\" / \"test_predictions.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "939d1d18-9b43-453f-be30-bc9d9fc930c2",
   "metadata": {},
   "source": [
    "## Model Evaluation\n",
    "\n",
    " - Compare Simple model predictions vs ML Test data predictions\n",
    " - Compare OOF/Train data vs Test/Validation data\n",
    " - Compare early season Test data vs later season Test data\n",
    " \n",
    " Feature importances via Shapley values are *local* to the given dataset and can assist in adversarial validation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5bf591c7-b49e-4a19-8674-e02900e39a0e",
   "metadata": {},
   "source": [
    "**Split Test data**\n",
    "\n",
    "Compare the model performance on the early part of the test data vs the later part of the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf71181e",
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_NAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d9e9172-4782-4bfe-8679-655d016d0837",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv(DATA_PATH / \"processed\" / TEST_NAME)\n",
    "test = fix_datatypes(test, DATE_FIELDS, SHORT_INTEGER_FIELDS, LONG_INTEGER_FIELDS)\n",
    "test = encode_categoricals(test, CATEGORY_COLUMNS, MODEL_NAME, ENABLE_CATEGORICAL)\n",
    "\n",
    "num_of_rows = test.shape[0]\n",
    "test = test.sort_values(by=[\"GAME_DATE_EST\"])  # sort the data by date\n",
    "SPLIT = test.iloc[num_of_rows // 2][\n",
    "    \"GAME_DATE_EST\"\n",
    "]  # split the data in half and find the date in the middle\n",
    "\n",
    "\n",
    "run[\"test_split_Test1/end_date\"] = SPLIT\n",
    "run[\"test_split_Test2/start_date\"] = SPLIT\n",
    "\n",
    "test1 = test[test[\"GAME_DATE_EST\"] < SPLIT]\n",
    "test2 = test[test[\"GAME_DATE_EST\"] >= SPLIT]\n",
    "\n",
    "test1_target = test1[\"TARGET\"]\n",
    "test2_target = test2[\"TARGET\"]\n",
    "\n",
    "\n",
    "test1 = test1[use_columns]\n",
    "test2 = test2[use_columns]\n",
    "\n",
    "test1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5e6d977-d7e6-4b5f-9139-f3b36ab4ff57",
   "metadata": {},
   "source": [
    "**Process Splits**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63a9fb0f-f8b0-417c-8013-4a6b89ead655",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_splits(label, test, test_target, results):\n",
    "\n",
    "    if MODEL_NAME == \"xgboost\":\n",
    "        test_data = xgb.DMatrix(test, enable_categorical=ENABLE_CATEGORICAL)\n",
    "    if MODEL_NAME == \"lightgbm\":\n",
    "        test_data = test\n",
    "\n",
    "    test_preds = model.predict(test_data)\n",
    "    test_preds_shap = get_shapley(MODEL_NAME, model, test_data)\n",
    "\n",
    "    test_preds_binary, acc_score, auc_score, optimal_threshold = get_scores(\n",
    "        test_target, test_preds\n",
    "    )\n",
    "\n",
    "    run[\"test_split_\" + label + \"/accuracy\"] = acc_score\n",
    "    run[\"test_split_\" + label + \"/AUC\"] = auc_score\n",
    "    run[\"test_split_\" + label + \"/optimal_threshold\"] = optimal_threshold\n",
    "\n",
    "    df = {\n",
    "        \"Label\": label,\n",
    "        \"Accuracy\": acc_score,\n",
    "        \"AUC\": auc_score,\n",
    "        \"Threshold\": optimal_threshold,\n",
    "    }\n",
    "    results.loc[len(results)] = [label, acc_score, auc_score, optimal_threshold]\n",
    "\n",
    "    run[\"test_split_\" + label + \"/classification_report\"] = classification_report(\n",
    "        test_target, test_preds_binary\n",
    "    )\n",
    "    print(classification_report(test_target, test_preds_binary))\n",
    "\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(10, 10))\n",
    "    shap.summary_plot(test_preds_shap[:, :-1], test, plot_type=\"bar\")\n",
    "    # run[\"test_split_\" + label + \"/shapley_summary_bar\"].upload(fig)\n",
    "\n",
    "    # Simple model applied to split\n",
    "    acc_score, auc_score = SimpleModel(test, test_target)\n",
    "    df = {\n",
    "        \"Label\": \"Simple-\" + label,\n",
    "        \"Accuracy\": acc_score,\n",
    "        \"AUC\": auc_score,\n",
    "        \"Threshold\": \"N/A\",\n",
    "    }\n",
    "    results.loc[len(results)] = [\"Simple-\" + label, acc_score, auc_score, \"N/A\"]\n",
    "\n",
    "    return test_preds_shap, results\n",
    "\n",
    "\n",
    "print(\"TEST1\")\n",
    "test_preds_shap1, results = process_splits(\"Test1\", test1, test1_target, results)\n",
    "print(\"TEST2\")\n",
    "test_preds_shap2, results = process_splits(\"Test2\", test2, test2_target, results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a044a147-183f-4b56-8c54-2cf462d65b9d",
   "metadata": {},
   "source": [
    "**Summary Table**\n",
    "\n",
    "Key metrics from Simple Model, Train, Test, and Test-split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2322001a-4a56-4832-aa97-092681ccac43",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_score1 = results.loc[results[\"Label\"] == \"Train\", \"Accuracy\"].values[0]\n",
    "acc_score2 = results.loc[results[\"Label\"] == \"Test\", \"Accuracy\"].values[0]\n",
    "acc_score = acc_score1 - acc_score2\n",
    "auc_score1 = results.loc[results[\"Label\"] == \"Train\", \"AUC\"].values[0]\n",
    "auc_score2 = results.loc[results[\"Label\"] == \"Test\", \"AUC\"].values[0]\n",
    "auc_score = auc_score1 - auc_score2\n",
    "\n",
    "df = {\n",
    "    \"Label\": \"Train-Test\",\n",
    "    \"Accuracy\": acc_score,\n",
    "    \"AUC\": auc_score,\n",
    "    \"Threshold\": \"N/A\",\n",
    "}\n",
    "results.loc[len(results)] = [\"Train-Test\", acc_score, auc_score, \"N/A\"]\n",
    "\n",
    "\n",
    "run[\"evaluation/train-test_accuracy\"] = acc_score\n",
    "run[\"evaluation/train-test_AUC\"] = auc_score\n",
    "\n",
    "acc_score1 = results.loc[results[\"Label\"] == \"Test1\", \"Accuracy\"].values[0]\n",
    "acc_score2 = results.loc[results[\"Label\"] == \"Test2\", \"Accuracy\"].values[0]\n",
    "acc_score = acc_score1 - acc_score2\n",
    "auc_score1 = results.loc[results[\"Label\"] == \"Test1\", \"AUC\"].values[0]\n",
    "auc_score2 = results.loc[results[\"Label\"] == \"Test2\", \"AUC\"].values[0]\n",
    "auc_score = auc_score1 - auc_score2\n",
    "\n",
    "df = {\n",
    "    \"Label\": \"Test1-Test2\",\n",
    "    \"Accuracy\": acc_score,\n",
    "    \"AUC\": auc_score,\n",
    "    \"Threshold\": \"N/A\",\n",
    "}\n",
    "results.loc[len(results)] = [\"Test1-Test2\", acc_score, auc_score, \"N/A\"]\n",
    "\n",
    "\n",
    "run[\"evaluation/test1-test2_accuracy\"] = acc_score\n",
    "run[\"evaluation/test1-test2_AUC\"] = auc_score\n",
    "\n",
    "run[\"evaluation/summary_table\"].upload(File.as_html(results))\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5da94d8-1bbe-4925-9ab5-ec441078bc12",
   "metadata": {},
   "source": [
    "**Train vs Test Feature Importances via Shapley Values**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cab07c41-c127-4d3e-97e8-fe06ba7c2ff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(13, 7))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.title(\"Train vs Test Shapley Summary Bar\")\n",
    "shap.summary_plot(\n",
    "    train_oof_shap[:, :-1],\n",
    "    train[use_columns],\n",
    "    plot_type=\"bar\",\n",
    "    plot_size=None,\n",
    "    show=False,\n",
    ")\n",
    "plt.subplot(1, 2, 2)\n",
    "shap.summary_plot(\n",
    "    test_preds_shap[:, :-1],\n",
    "    test1[use_columns],\n",
    "    plot_type=\"bar\",\n",
    "    plot_size=None,\n",
    "    show=False,\n",
    ")\n",
    "plt.tight_layout()\n",
    "\n",
    "fig.show()\n",
    "# run[\"evaluation/test_train_shapley_summary_bar\"].upload(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55dd7e27-db01-4930-bf1a-d33034ab95fe",
   "metadata": {},
   "source": [
    "**Test1 vs Test2 Feature Importances via Shapley Values**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b5e0802-16ce-4ba7-9b06-df392bd67215",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(13, 7))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.title(\"Test 1 vs Test 2 Shapley Summary Bar\")\n",
    "shap.summary_plot(\n",
    "    test_preds_shap1[:, :-1],\n",
    "    test1[use_columns],\n",
    "    plot_type=\"bar\",\n",
    "    plot_size=None,\n",
    "    show=False,\n",
    ")\n",
    "plt.subplot(1, 2, 2)\n",
    "shap.summary_plot(\n",
    "    test_preds_shap2[:, :-1],\n",
    "    test1[use_columns],\n",
    "    plot_type=\"bar\",\n",
    "    plot_size=None,\n",
    "    show=False,\n",
    ")\n",
    "plt.tight_layout()\n",
    "fig.show()\n",
    "# run[\"evaluation/test1_test2_shapley_summary_bar\"].upload(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35207419-7d26-4bd1-800b-f9969e2d9ad4",
   "metadata": {},
   "source": [
    "**End experiment tracking**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0469ee75-dc3f-4951-88cf-5f9987a71391",
   "metadata": {},
   "outputs": [],
   "source": [
    "# end experiment tracking\n",
    "run.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6a06d31-2ca8-4b0f-98f0-dbe6a4892e70",
   "metadata": {},
   "source": [
    "**Save Tuned Hyperparameters**\n",
    "\n",
    "Optional function - when run, it overwrites the current \"best\" hyperparameters with the newly generated hyperparameters. Some descretion is required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4474f7f2-edb5-43b8-9ac8-ef80f71fb36e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_tuned_params(MODEL_NAME, tuned_params):\n",
    "    with open(CONFIGS_PATH / (MODEL_NAME + \".json\"), \"w\") as f:\n",
    "        f.write(json.dumps(tuned_params))\n",
    "\n",
    "\n",
    "save_tuned_params(MODEL_NAME, tuned_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "614fc6fa-8c4f-4fc2-8cd5-63c5a19f1483",
   "metadata": {},
   "source": [
    "**Visualize Data Comparisons**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2af3ffb-bca4-443d-8c4a-8d613e8443d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run Sweetviz comparing test data that led to a correct prediction vs incorrect prediction\n",
    "\n",
    "test = pd.read_csv(DATA_PATH / \"processed\" / \"test_predictions.csv\")\n",
    "\n",
    "test[\"TARGET\"] = test[\"TARGET\"].astype(\"int8\")\n",
    "test[\"Prediction\"] = test[\"Prediction\"].astype(\"int8\")\n",
    "\n",
    "test_correct = test[test[\"TARGET\"] == test[\"Prediction\"]]\n",
    "test_wrong = test[test[\"TARGET\"] != test[\"Prediction\"]]\n",
    "\n",
    "# run_sweetviz_comparison(test_correct, 'Test-Correct', test_wrong, 'Test-Wrong', 'Prediction', 'correct-incorrect')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2f50df74",
   "metadata": {},
   "source": [
    "**Calibrate Probabilities**\n",
    "\n",
    "The goal of the prediction model is to predict the probability of a win or loss for each game. We need to make sure that the model is predicting probabilities that are in line with the actual win/loss outcomes. The proba predictions from most classifiers do not typically align with the actual outcomes. Calibration fits a function to the model's probabilities and the actual outcomes.\n",
    "\n",
    "The first step is to plot the actual probability distribution of the base model against a perfectly calibrated ideal distribution. In some cases, the base model may be sufficient on its own, but it can often be improved by calibrating the probabilities.\n",
    "\n",
    "Next, we apply SKLearns built-in functions to calibrate the model's probabilities. This is done by fitting both an isotonic regression model and a sigmoid regression model to the model's probabilities and the actual outcomes. These fitted curves represent calibrated probabilities.\n",
    "\n",
    "By plotting the graph, we can see how well the model's probabilities are calibrated. The ideal distribution is represented by the dotted line, and the model \"closest\" to this line is the best calibrated model.\n",
    "\n",
    "Brier loss is typically used to numerically measure how well the model is calibrated. It is a measure of the mean squared difference between the predicted probabilities and the actual outcomes. The lower the Brier loss, the better the calibration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff02b950",
   "metadata": {},
   "outputs": [],
   "source": [
    "# since we are using SKLearn's built in functions to to calibrate and plot the probability curves,\n",
    "# we first need to convert our model to one using a SKLearn wrapper.\n",
    "if MODEL_NAME == \"xgboost\":\n",
    "    model = XGBClassifier(n_estimators=NUM_BOOST_ROUND, **model_params)\n",
    "if MODEL_NAME == \"lightgbm\":\n",
    "    model = LGBMClassifier(verbose_eval=False, **model_params)\n",
    "\n",
    "# we then set up CalibratedClassifierCV using Isotonic and Sigmoid Regression\n",
    "model_isotonic = CalibratedClassifierCV(model, cv=5, method=\"isotonic\")\n",
    "model_sigmoid = CalibratedClassifierCV(model, cv=5, method=\"sigmoid\")\n",
    "\n",
    "clf_list = [\n",
    "    (model, \"Base Model\"),\n",
    "    (model_isotonic, \"Model + Isotonic\"),\n",
    "    (model_sigmoid, \"Model + Sigmoid\"),\n",
    "]\n",
    "\n",
    "y_train = target\n",
    "y_test = test_target_original\n",
    "\n",
    "X_train = train[use_columns]\n",
    "X_test = test_original\n",
    "\n",
    "plot_calibration_curve(clf_list, X_train, y_train, X_test, y_test, n_bins=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53a4b541",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate classification metrics we will use to compare the models\n",
    "# and return an updated clf_list that includes the trained/fitted models\n",
    "df_scores, clf_list = calculate_classification_metrics(\n",
    "    clf_list, X_train, y_train, X_test, y_test\n",
    ")\n",
    "df_scores = df_scores.reset_index()\n",
    "df_scores"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "bf2538e1",
   "metadata": {},
   "source": [
    "**Select Best Model Calibration and Save**\n",
    "\n",
    "Select the model with the lowest Brier loss score and save the calibrated model to a pickle file. This is the model that will be used for the final predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4380bcab",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_best = df_scores.loc[df_scores[\"Brier  loss\"].idxmin()]\n",
    "best_calibrated_model = df_best[\"Classifier\"]\n",
    "\n",
    "print(\"Best calibrated model is: \", best_calibrated_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff675ed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the pickeled model and key metrics so that it can be saved to the model registry later on\n",
    "\n",
    "\n",
    "# retrieve the AUC and Accuracy scores for the test set from the summary table we created earlier named 'results'\n",
    "test_results = results.loc[results[\"Label\"] == \"Test\"]\n",
    "# auc_score = test_results['AUC'].values[0]\n",
    "# acc_score = test_results['Accuracy'].values[0]\n",
    "\n",
    "# select the best calibrated model from the list of models we created earlier\n",
    "model = [\n",
    "    classifier for (classifier, name) in clf_list if name == best_calibrated_model\n",
    "][0]\n",
    "\n",
    "\n",
    "joblib.dump(model, MODELS_PATH / \"model.pkl\")\n",
    "\n",
    "model_data = {\n",
    "    \"model_name\": MODEL_NAME,\n",
    "    \"calibration_method\": best_calibrated_model,\n",
    "    \"brier_loss\": df_best[\"Brier  loss\"],\n",
    "    # 'metrics':{'AUC': auc_score, 'Accuracy': acc_score },\n",
    "}\n",
    "\n",
    "with open(MODELS_PATH / \"model_data.json\", \"w\") as f:\n",
    "    f.write(json.dumps(model_data))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "basketball-prediction-klrE3EAW-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
